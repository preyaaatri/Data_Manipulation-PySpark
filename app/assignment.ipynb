{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'org'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ab85e9e59de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataTypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'org'"
     ]
    }
   ],
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.types.DataTypes\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class StructType(fields: Array[StructField])\n",
    "\n",
    "case class StructField(\n",
    "        name: String,\n",
    "        dataType: DataType,\n",
    "        nullable: Boolean = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object Fda {\n",
    "    \n",
    "    def getType(raw: Any): DataType = {\n",
    "      raw match {\n",
    "        case \"byte\" => ByteType\n",
    "        case \"short\" => ShortType\n",
    "        case \"integer\" => IntegerType\n",
    "        case \"long\" => LongType\n",
    "        case \"float\" => FloatType\n",
    "        case \"number\" => DoubleType\n",
    "        case \"boolean\" => BooleanType\n",
    "        case \"datetime\" => TimestampType\n",
    "        case _ => StringType\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def main(args: Array[String]): Unit = {\n",
    "        val spark = SparkSession\n",
    "          .builder()\n",
    "          .appName(\"PreyAssignment\")\n",
    "          .config(\"spark.some.config.option\", \"some-value\")\n",
    "          .getOrCreate()\n",
    "        \n",
    "        val filePath = \"./DM-classification.json\"\n",
    "        val df = spark.read.option(\"multiline\", true).json(\"./DM-classification.json\")\n",
    "        \n",
    "        \n",
    "        val col_list = df.select(explode(df(\"schema.fields\"))).toDF(\"level1\")\n",
    "                          .select(\"level1.type\")\n",
    "        val list1 = col_list.select(\"type\").collect().map(_(0)).toList\n",
    "        \n",
    "        println(list1(0))\n",
    "        println(list1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        val tf = df.select(explode(df(\"data\"))).toDF(\"temp\")\n",
    "        .select(\"temp.content\", \"temp.label\", \"temp.label_1\", \"temp.label_2\", \"temp.label_3\", \"temp.label_4\")\n",
    "        \n",
    "        val new_columns = Seq(\"content\",\"label\",\"size\",\"usage\",\"effect\",\"date\")\n",
    "        \n",
    "        val new_df = tf.toDF(new_columns:_*)\n",
    "        new_df.show()\n",
    "        \n",
    "        \n",
    "        val newSeqOfSeq = new_df.collect().map(row => row.toSeq.map(_.toString).toSeq).toSeq\n",
    "        \n",
    "        println(newSeqOfSeq)\n",
    "        \n",
    "        \n",
    "        \n",
    "        val simpleSchema = StructType(Array(\n",
    "            StructField(\"content\",getType(list1(1)),true),\n",
    "            StructField(\"label\",getType(list1(2)),true),\n",
    "            StructField(\"size\",getType(list1(3)),true),\n",
    "            StructField(\"usage\", getType(list1(4)), true),\n",
    "            StructField(\"effect\", getType(list1(5)), true),\n",
    "            StructField(\"date\", getType(list1(6)), true)\n",
    "          ))\n",
    "             \n",
    "        \n",
    "        val final_df = spark.createDataFrame(spark.sparkContext.parallelize(newSeqOfSeq),simpleSchema)\n",
    "        final_df.printSchema()\n",
    "        final_df.show()\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fda.main(Array(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}