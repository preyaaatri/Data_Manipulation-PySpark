{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, unix_timestamp, col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType, DoubleType, ByteType, ShortType, LongType, FloatType, BooleanType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getType(raw):\n",
    "    switch = {  \n",
    "    \"byte\": ByteType(),\n",
    "    \"short\": ShortType(),\n",
    "    \"integer\": IntegerType(),\n",
    "    \"long\": LongType(),\n",
    "    \"float\": FloatType(),\n",
    "    \"number\": DoubleType(),\n",
    "    \"boolean\": BooleanType(),\n",
    "    \"datetime\": TimestampType(),\n",
    "    }\n",
    "    return switch.get(raw, StringType())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\", True).json(\"./DM-classification.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: long (nullable = true)\n",
      " |    |    |-- label_1: string (nullable = true)\n",
      " |    |    |-- label_2: string (nullable = true)\n",
      " |    |    |-- label_3: double (nullable = true)\n",
      " |    |    |-- label_4: string (nullable = true)\n",
      " |-- schema: struct (nullable = true)\n",
      " |    |-- fields: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |-- pandas_version: string (nullable = true)\n",
      " |    |-- primaryKey: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                data|              schema|\n",
      "+--------------------+--------------------+\n",
      "|[[The battery is ...|[[[index, integer...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "|             content|label|label_1| label_2|     label_3|           datetime|\n",
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "|The battery is co...|    0|  small|separate|0.7155163569|2015-06-05 18:41:08|\n",
      "|What a big waste ...|    0| medium|conected| 0.858630808|2016-10-29 12:12:46|\n",
      "|Don't waste your ...|    0|  large|conected|0.2040485858|2016-04-29 14:44:31|\n",
      "|Great sound and s...|    1|  large|separate| 0.332641236|2017-12-26 13:25:48|\n",
      "|Really pleased wi...|    1| medium|conected| 0.887390017|2016-04-30 00:01:08|\n",
      "|One of my favorit...|    1|  large|conected|0.2305351126|2016-04-30 17:29:03|\n",
      "|best bluetooth on...|    1| medium|conected|0.4549175852|2017-04-24 04:26:54|\n",
      "|Authentic leather...|    1|  large|conected|0.3198441525|2015-12-16 22:03:11|\n",
      "|I was very excite...|    1| medium|conected| 0.835863266|2015-05-19 01:34:19|\n",
      "|Do not make the s...|    0|  small|conected|0.1442302304|2015-02-03 03:31:18|\n",
      "|Big Disappointmen...|    0|  large|separate|0.8193389616|2017-11-26 06:01:00|\n",
      "|the phone was unu...|    0|  large|separate| 0.136951624|2016-10-20 13:10:48|\n",
      "|Worst Customer Se...|    0|  small|conected|0.8700157352|2015-11-09 10:40:50|\n",
      "|No additional ear...|    0|  large|separate| 0.719855014|2017-02-17 19:49:15|\n",
      "|It defeats the pu...|    0|  small|conected|0.7345972022|2015-06-22 01:20:10|\n",
      "|  Worth every penny.|    1|  large|separate|0.5062303093|2017-09-15 10:03:16|\n",
      "|Excellent wallet ...|    1|  small|conected|0.4407609104|2015-06-27 00:04:24|\n",
      "|Nice headphones f...|    1| medium|conected|0.5066209266|2017-01-01 00:32:42|\n",
      "|Internet is excru...|    0| medium|conected|0.5714815553|2015-02-06 17:18:37|\n",
      "|It is very comfor...|    1|  small|separate|0.9208235177|2015-04-30 03:07:00|\n",
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf = df.select(explode(df[\"data\"])).toDF(\"temp\").select(\"temp.content\", \"temp.label\", \"temp.label_1\", \"temp.label_2\", \"temp.label_3\", \"temp.label_4\")\n",
    "# tf = tf.withColumn(\"datetime\", col(tf.label_4).cast('timestamp'))\n",
    "\n",
    "\n",
    "tf = df.select(explode(df[\"data\"])).toDF(\"temp\").select(\"temp.content\", \"temp.label\", \"temp.label_1\", \"temp.label_2\", \"temp.label_3\", \"temp.label_4\")\n",
    "tf = tf.withColumn(\"datetime\", to_timestamp(unix_timestamp(col('label_4'),\"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\").cast('timestamp')))\n",
    "tf = tf.drop(\"label_4\")\n",
    "\n",
    "tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = list(tf.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['integer', 'string', 'integer', 'string', 'string', 'number', 'datetime']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DoubleType"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = df.select(explode(df[\"schema.fields\"])).toDF(\"level1\").select(\"level1.type\")\n",
    "datatype_list = [row[0] for row in col_list.select(\"type\").collect()]\n",
    "\n",
    "print(datatype_list)\n",
    "\n",
    "getType(datatype_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSchema = StructType([\n",
    "    StructField(\"content\", getType(datatype_list[1]),True),\n",
    "    StructField(\"label\", getType(datatype_list[2]),True),\n",
    "    StructField(\"size\", getType(datatype_list[3]),True),\n",
    "    StructField(\"usage\", getType(datatype_list[4]),True),\n",
    "    StructField(\"effect\", getType(datatype_list[5]),True),\n",
    "    StructField(\"date\", getType(datatype_list[6]),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(data=simpleData, schema=simpleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- effect: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|             content|label|  size|   usage|      effect|               date|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|The battery is co...|    0| small|separate|0.7155163569|2015-06-05 18:41:08|\n",
      "|What a big waste ...|    0|medium|conected| 0.858630808|2016-10-29 12:12:46|\n",
      "|Don't waste your ...|    0| large|conected|0.2040485858|2016-04-29 14:44:31|\n",
      "|Great sound and s...|    1| large|separate| 0.332641236|2017-12-26 13:25:48|\n",
      "|Really pleased wi...|    1|medium|conected| 0.887390017|2016-04-30 00:01:08|\n",
      "|One of my favorit...|    1| large|conected|0.2305351126|2016-04-30 17:29:03|\n",
      "|best bluetooth on...|    1|medium|conected|0.4549175852|2017-04-24 04:26:54|\n",
      "|Authentic leather...|    1| large|conected|0.3198441525|2015-12-16 22:03:11|\n",
      "|I was very excite...|    1|medium|conected| 0.835863266|2015-05-19 01:34:19|\n",
      "|Do not make the s...|    0| small|conected|0.1442302304|2015-02-03 03:31:18|\n",
      "|Big Disappointmen...|    0| large|separate|0.8193389616|2017-11-26 06:01:00|\n",
      "|the phone was unu...|    0| large|separate| 0.136951624|2016-10-20 13:10:48|\n",
      "|Worst Customer Se...|    0| small|conected|0.8700157352|2015-11-09 10:40:50|\n",
      "|No additional ear...|    0| large|separate| 0.719855014|2017-02-17 19:49:15|\n",
      "|It defeats the pu...|    0| small|conected|0.7345972022|2015-06-22 01:20:10|\n",
      "|  Worth every penny.|    1| large|separate|0.5062303093|2017-09-15 10:03:16|\n",
      "|Excellent wallet ...|    1| small|conected|0.4407609104|2015-06-27 00:04:24|\n",
      "|Nice headphones f...|    1|medium|conected|0.5066209266|2017-01-01 00:32:42|\n",
      "|Internet is excru...|    0|medium|conected|0.5714815553|2015-02-06 17:18:37|\n",
      "|It is very comfor...|    1| small|separate|0.9208235177|2015-04-30 03:07:00|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.createOrReplaceTempView(\"DM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- effect: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- diff_date: interval (nullable = true)\n",
      "\n",
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- effect: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- diff_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selectdf = spark.sql(\"Select * from DM where label=1\")\n",
    "groupedby =spark.sql(\"SELECT *, date - lag(date,1) OVER(PARTITION BY size order by date) as diff_date FROM DM\")\n",
    "groupedby.printSchema()\n",
    "# print(type(groupedby[\"diff_date\"][0]))\n",
    "groupedby = groupedby.withColumn(\"diff_date\", col(\"diff_date\").cast('String'))\n",
    "groupedby.printSchema()\n",
    "# groupedby.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedby.repartition(1).write.format('csv').save(\"./output/myfile1.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
