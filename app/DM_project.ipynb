{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, unix_timestamp, col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType, DoubleType, ByteType, ShortType, LongType, FloatType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SparkSession has been setup to create a DataFrame, register DataFrame as views, execute SQL over tables, read JSON files.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Data Frame extraction\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: long (nullable = true)\n",
      " |    |    |-- label_1: string (nullable = true)\n",
      " |    |    |-- label_2: string (nullable = true)\n",
      " |    |    |-- label_3: double (nullable = true)\n",
      " |    |    |-- label_4: string (nullable = true)\n",
      " |-- schema: struct (nullable = true)\n",
      " |    |-- fields: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |-- pandas_version: string (nullable = true)\n",
      " |    |-- primaryKey: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                data|              schema|\n",
      "+--------------------+--------------------+\n",
      "|[[The battery is ...|[[[index, integer...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read the nested JSON file\n",
    "df = spark.read.option(\"multiline\", True).json(\"./DM-classification.json\")\n",
    "\n",
    "## Print the schema of the file that has been read.\n",
    "df.printSchema()\n",
    "\n",
    "## Print the data frame.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "|             content|label|label_1| label_2|     label_3|           datetime|\n",
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "|The battery is co...|    0|  small|separate|0.7155163569|2015-06-05 18:41:08|\n",
      "|What a big waste ...|    0| medium|conected| 0.858630808|2016-10-29 12:12:46|\n",
      "|Don't waste your ...|    0|  large|conected|0.2040485858|2016-04-29 14:44:31|\n",
      "|Great sound and s...|    1|  large|separate| 0.332641236|2017-12-26 13:25:48|\n",
      "|Really pleased wi...|    1| medium|conected| 0.887390017|2016-04-30 00:01:08|\n",
      "|One of my favorit...|    1|  large|conected|0.2305351126|2016-04-30 17:29:03|\n",
      "|best bluetooth on...|    1| medium|conected|0.4549175852|2017-04-24 04:26:54|\n",
      "|Authentic leather...|    1|  large|conected|0.3198441525|2015-12-16 22:03:11|\n",
      "|I was very excite...|    1| medium|conected| 0.835863266|2015-05-19 01:34:19|\n",
      "|Do not make the s...|    0|  small|conected|0.1442302304|2015-02-03 03:31:18|\n",
      "|Big Disappointmen...|    0|  large|separate|0.8193389616|2017-11-26 06:01:00|\n",
      "|the phone was unu...|    0|  large|separate| 0.136951624|2016-10-20 13:10:48|\n",
      "|Worst Customer Se...|    0|  small|conected|0.8700157352|2015-11-09 10:40:50|\n",
      "|No additional ear...|    0|  large|separate| 0.719855014|2017-02-17 19:49:15|\n",
      "|It defeats the pu...|    0|  small|conected|0.7345972022|2015-06-22 01:20:10|\n",
      "|  Worth every penny.|    1|  large|separate|0.5062303093|2017-09-15 10:03:16|\n",
      "|Excellent wallet ...|    1|  small|conected|0.4407609104|2015-06-27 00:04:24|\n",
      "|Nice headphones f...|    1| medium|conected|0.5066209266|2017-01-01 00:32:42|\n",
      "|Internet is excru...|    0| medium|conected|0.5714815553|2015-02-06 17:18:37|\n",
      "|It is very comfor...|    1|  small|separate|0.9208235177|2015-04-30 03:07:00|\n",
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Exploding the data column of the dataframe that has been extracted, in order to obtain the content and the data labels.\n",
    "data_df = df.select(explode(df[\"data\"])).toDF(\"temp\").select(\"temp.content\", \"temp.label\", \"temp.label_1\", \"temp.label_2\", \"temp.label_3\", \"temp.label_4\")\n",
    "\n",
    "## Creating a new column having datetime datatype and the content of label_4.\n",
    "data_df = data_df.withColumn(\"datetime\", to_timestamp(unix_timestamp(col('label_4'),\"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\").cast('timestamp')))\n",
    "\n",
    "## Dropping label_4 from the dataframe.\n",
    "data_df = data_df.drop(\"label_4\")\n",
    "\n",
    "## Print the data frame after adding and dropping a column.\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of all the rows that are present in the dataframe obtained after the above modifications have been made.  \n",
    "Data = list(data_df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    type|\n",
      "+--------+\n",
      "| integer|\n",
      "|  string|\n",
      "| integer|\n",
      "|  string|\n",
      "|  string|\n",
      "|  number|\n",
      "|datetime|\n",
      "+--------+\n",
      "\n",
      "['integer', 'string', 'integer', 'string', 'string', 'number', 'datetime']\n"
     ]
    }
   ],
   "source": [
    "## Explode the fields part of the dataframe to obtain the data types of all the columns.\n",
    "col_list = df.select(explode(df[\"schema.fields\"])).toDF(\"level1\").select(\"level1.type\")\n",
    "col_list.show()\n",
    "\n",
    "## Extracting the above obtained datatypes to a list.\n",
    "datatype_list = [row[0] for row in col_list.select(\"type\").collect()]\n",
    "print(datatype_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to obtain the datatype from the JSON data that is avaialble.\n",
    "def getType(raw):\n",
    "    switch = {  \n",
    "    \"byte\": ByteType(),\n",
    "    \"short\": ShortType(),\n",
    "    \"integer\": IntegerType(),\n",
    "    \"long\": LongType(),\n",
    "    \"float\": FloatType(),\n",
    "    \"number\": DoubleType(),\n",
    "    \"boolean\": BooleanType(),\n",
    "    \"datetime\": TimestampType(),\n",
    "    }\n",
    "    return switch.get(raw, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a schema containing column names as per the question and data types as per the JSON data that was available and finally extracted to a list in the above steps.\n",
    "## The getType function has been used to convert the field data available in string to datatype type.\n",
    "Schema = StructType([\n",
    "    StructField(\"content\", getType(datatype_list[1]),True),\n",
    "    StructField(\"label\", getType(datatype_list[2]),True),\n",
    "    StructField(\"size\", getType(datatype_list[3]),True),\n",
    "    StructField(\"usage\", getType(datatype_list[4]),True),\n",
    "    StructField(\"effect\", getType(datatype_list[5]),True),\n",
    "    StructField(\"date\", getType(datatype_list[6]),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- effect: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|             content|label|  size|   usage|      effect|               date|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|The battery is co...|    0| small|separate|0.7155163569|2015-06-05 18:41:08|\n",
      "|What a big waste ...|    0|medium|conected| 0.858630808|2016-10-29 12:12:46|\n",
      "|Don't waste your ...|    0| large|conected|0.2040485858|2016-04-29 14:44:31|\n",
      "|Great sound and s...|    1| large|separate| 0.332641236|2017-12-26 13:25:48|\n",
      "|Really pleased wi...|    1|medium|conected| 0.887390017|2016-04-30 00:01:08|\n",
      "|One of my favorit...|    1| large|conected|0.2305351126|2016-04-30 17:29:03|\n",
      "|best bluetooth on...|    1|medium|conected|0.4549175852|2017-04-24 04:26:54|\n",
      "|Authentic leather...|    1| large|conected|0.3198441525|2015-12-16 22:03:11|\n",
      "|I was very excite...|    1|medium|conected| 0.835863266|2015-05-19 01:34:19|\n",
      "|Do not make the s...|    0| small|conected|0.1442302304|2015-02-03 03:31:18|\n",
      "|Big Disappointmen...|    0| large|separate|0.8193389616|2017-11-26 06:01:00|\n",
      "|the phone was unu...|    0| large|separate| 0.136951624|2016-10-20 13:10:48|\n",
      "|Worst Customer Se...|    0| small|conected|0.8700157352|2015-11-09 10:40:50|\n",
      "|No additional ear...|    0| large|separate| 0.719855014|2017-02-17 19:49:15|\n",
      "|It defeats the pu...|    0| small|conected|0.7345972022|2015-06-22 01:20:10|\n",
      "|  Worth every penny.|    1| large|separate|0.5062303093|2017-09-15 10:03:16|\n",
      "|Excellent wallet ...|    1| small|conected|0.4407609104|2015-06-27 00:04:24|\n",
      "|Nice headphones f...|    1|medium|conected|0.5066209266|2017-01-01 00:32:42|\n",
      "|Internet is excru...|    0|medium|conected|0.5714815553|2015-02-06 17:18:37|\n",
      "|It is very comfor...|    1| small|separate|0.9208235177|2015-04-30 03:07:00|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create a dataframe by combining the list of rows and the schema that have \n",
    "final_df = spark.createDataFrame(data=Data, schema=Schema)\n",
    "\n",
    "## Print the schema of the data frame that has been created with the modified column names and data types.\n",
    "final_df.printSchema()\n",
    "\n",
    "## Print the data frame that has thus been obtained.\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new temporary view using a SparkDataFrame in the Spark Session. \n",
    "## If a temporary view with the same name already exists, it is replaced.\n",
    "final_df.createOrReplaceTempView(\"DM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- effect: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- diff_date: string (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+--------+------------+-------------------+--------------------+\n",
      "|             content|label|  size|   usage|      effect|               date|           diff_date|\n",
      "+--------------------+-----+------+--------+------------+-------------------+--------------------+\n",
      "|Better than you'd...|    1|medium|separate| 0.049132322|2015-01-01 23:54:55|                null|\n",
      "|I love the look a...|    1|medium|separate|0.5040957229|2015-01-04 04:11:57|52 hours 17 minut...|\n",
      "|lightweight and w...|    1|medium|conected|0.2213363547|2015-01-13 15:30:56|227 hours 18 minu...|\n",
      "|EXCELLENT SERVICE...|    1|medium|separate|0.6684907436|2015-01-14 23:04:14|31 hours 33 minut...|\n",
      "|Everything about ...|    1|medium|conected|0.1309561045|2015-01-15 20:07:22|21 hours 3 minute...|\n",
      "|I get absolutely ...|    0|medium|separate|0.9481961624|2015-01-16 06:24:17|10 hours 16 minut...|\n",
      "|Five star Plus, p...|    1|medium|conected|0.1540949024|2015-01-17 22:36:15|40 hours 11 minut...|\n",
      "|The range is very...|    1|medium|separate|0.0240287714|2015-01-28 01:30:35|242 hours 54 minu...|\n",
      "|My father has the...|    0|medium|conected|0.2127243987|2015-01-30 18:12:41|64 hours 42 minut...|\n",
      "|   It was horrible!.|    0|medium|conected|0.0528493387|2015-01-31 19:50:15|25 hours 37 minut...|\n",
      "|Light weight, I h...|    1|medium|separate|0.4133420385|2015-02-02 14:57:52|43 hours 7 minute...|\n",
      "|   Never got it!!!!!|    0|medium|separate|0.9879631688|2015-02-03 04:55:47|13 hours 57 minut...|\n",
      "|Internet is excru...|    0|medium|conected|0.5714815553|2015-02-06 17:18:37|84 hours 22 minut...|\n",
      "|horrible, had to ...|    0|medium|separate|0.6361942279|2015-02-15 23:23:28|222 hours 4 minut...|\n",
      "|Lasted one day an...|    0|medium|conected|0.9875535516|2015-02-20 04:54:09|101 hours 30 minu...|\n",
      "|Love it.. Great a...|    1|medium|separate|0.8991056524|2015-02-21 16:48:09| 35 hours 54 minutes|\n",
      "|Their Research an...|    1|medium|separate|0.4134599587|2015-02-24 15:18:05|70 hours 29 minut...|\n",
      "|Sound quality on ...|    1|medium|separate| 0.047711758|2015-02-26 02:36:24|35 hours 18 minut...|\n",
      "|If you like a lou...|    0|medium|separate|0.0933155848|2015-02-27 20:12:57|41 hours 36 minut...|\n",
      "|         Worst ever.|    0|medium|separate|0.7570866283|2015-03-03 17:04:44|92 hours 51 minut...|\n",
      "+--------------------+-----+------+--------+------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Grouping the table by 'size', and sort based on 'date' and extract difference between 'date' in consecutive records'\n",
    "groupedby =spark.sql(\"SELECT *, date - lag(date,1) OVER(PARTITION BY size order by date) as diff_date FROM DM\")\n",
    "\n",
    "## Creating a new column that is the difference between 'date' in consecutive records (within groups).\n",
    "groupedby = groupedby.withColumn(\"diff_date\", col(\"diff_date\").cast('String'))\n",
    "groupedby.printSchema()\n",
    "groupedby.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/jovyan/work/output/Manipulated_data.csv already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-e81699578dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Saving the above obtained data into a CSV file. The level of parallelism can be increased for better efficiency by changing the parameter in repartition option.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgroupedby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./output/Manipulated_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/jovyan/work/output/Manipulated_data.csv already exists.;"
     ]
    }
   ],
   "source": [
    "## Saving the above obtained data into a CSV file. The level of parallelism can be increased for better efficiency by changing the parameter in repartition option.\n",
    "groupedby.repartition(1).write.format('csv').save(\"./output/Manipulated_data.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
