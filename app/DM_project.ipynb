{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Manipulation (SparkSQL)\n",
    "\n",
    "Use Apache Spark to import the data: `./Data/classification/DM-classification.json`\n",
    "\n",
    "While you MUST use Apache Spark to answer this Data Manipulation question, ANY language (Scala, Pyspark, SparklyR) can be used.\n",
    "\n",
    "### Configure your environment\n",
    "\n",
    "Please explain this configuration, and why you chose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration and reason for the choice\n",
    "\n",
    "Jupyter - Development tool\n",
    "Docker - Environment virtualization\n",
    "PySpark - Programming language\n",
    "Spark - Cluster-computing framework\n",
    "\n",
    "Docker has to be installed for the project to run as this code runs inside a Docker container.\n",
    "Docker was used for the purpose of code portability and ease of programming.\n",
    "Firstly, the environment has to be setup by running the makefile.\n",
    "\n",
    "Command: make run\n",
    "\n",
    "The above command will spin up a docker container and provide you the link to use Jupyter notebook. Once you open the jupyter link, you can open DM_project.ipynb. By default the file will be opened in Python 3 kernal. Please change the kernal to spylon-kernal.\n",
    "\n",
    "Since the code is running inside a docker container, the data is not persistant. The sole purpose of this setup is to get your environment setup for running a pyspark project in jupyter.\n",
    "To deploy the project, you can run the code on the jupyter tool and save the file inside app folder to reuse it.\n",
    "\n",
    "The language used for the code is PySpark. The advantage of Pyspark is that Python has already many libraries for data science that can be plugged into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, unix_timestamp, col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType, DoubleType, ByteType, ShortType, LongType, FloatType, BooleanType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL\n",
    "\n",
    "Load the data into a Spark-SQL dataframe with the folowing columns: 'content','label','size','usage','effect','date'.  Create a Temporary Table for querying.\n",
    "\n",
    "Ensure you use appropriate types with your schema using: StructField, StructType."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data \n",
    "\n",
    "The data in the file was in a nested JSON format. The column names and their types have been extracted from the JSON file, alongwith the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SparkSession has been setup to create a DataFrame, register DataFrame as views, execute SQL over tables, read JSON files.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Data Frame extraction\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: long (nullable = true)\n",
      " |    |    |-- label_1: string (nullable = true)\n",
      " |    |    |-- label_2: string (nullable = true)\n",
      " |    |    |-- label_3: double (nullable = true)\n",
      " |    |    |-- label_4: string (nullable = true)\n",
      " |-- schema: struct (nullable = true)\n",
      " |    |-- fields: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |-- pandas_version: string (nullable = true)\n",
      " |    |-- primaryKey: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                data|              schema|\n",
      "+--------------------+--------------------+\n",
      "|[[The battery is ...|[[[index, integer...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read the nested JSON file\n",
    "df = spark.read.option(\"multiline\", True).json(\"./DM-classification.json\")\n",
    "\n",
    "## Print the schema of the file that has been read.\n",
    "df.printSchema()\n",
    "\n",
    "## Print the data frame.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output:\n",
    "All the content has been extracted from the JSON file using SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "|             content|label|label_1| label_2|     label_3|           datetime|\n",
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "|The battery is co...|    0|  small|separate|0.7155163569|2015-06-05 18:41:08|\n",
      "|What a big waste ...|    0| medium|conected| 0.858630808|2016-10-29 12:12:46|\n",
      "|Don't waste your ...|    0|  large|conected|0.2040485858|2016-04-29 14:44:31|\n",
      "|Great sound and s...|    1|  large|separate| 0.332641236|2017-12-26 13:25:48|\n",
      "|Really pleased wi...|    1| medium|conected| 0.887390017|2016-04-30 00:01:08|\n",
      "|One of my favorit...|    1|  large|conected|0.2305351126|2016-04-30 17:29:03|\n",
      "|best bluetooth on...|    1| medium|conected|0.4549175852|2017-04-24 04:26:54|\n",
      "|Authentic leather...|    1|  large|conected|0.3198441525|2015-12-16 22:03:11|\n",
      "|I was very excite...|    1| medium|conected| 0.835863266|2015-05-19 01:34:19|\n",
      "|Do not make the s...|    0|  small|conected|0.1442302304|2015-02-03 03:31:18|\n",
      "|Big Disappointmen...|    0|  large|separate|0.8193389616|2017-11-26 06:01:00|\n",
      "|the phone was unu...|    0|  large|separate| 0.136951624|2016-10-20 13:10:48|\n",
      "|Worst Customer Se...|    0|  small|conected|0.8700157352|2015-11-09 10:40:50|\n",
      "|No additional ear...|    0|  large|separate| 0.719855014|2017-02-17 19:49:15|\n",
      "|It defeats the pu...|    0|  small|conected|0.7345972022|2015-06-22 01:20:10|\n",
      "|  Worth every penny.|    1|  large|separate|0.5062303093|2017-09-15 10:03:16|\n",
      "|Excellent wallet ...|    1|  small|conected|0.4407609104|2015-06-27 00:04:24|\n",
      "|Nice headphones f...|    1| medium|conected|0.5066209266|2017-01-01 00:32:42|\n",
      "|Internet is excru...|    0| medium|conected|0.5714815553|2015-02-06 17:18:37|\n",
      "|It is very comfor...|    1|  small|separate|0.9208235177|2015-04-30 03:07:00|\n",
      "+--------------------+-----+-------+--------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Exploding the data column of the dataframe that has been extracted, in order to obtain the content and the data labels.\n",
    "data_df = df.select(explode(df[\"data\"])).toDF(\"temp\").select(\"temp.content\", \"temp.label\", \"temp.label_1\", \"temp.label_2\", \"temp.label_3\", \"temp.label_4\")\n",
    "\n",
    "## Creating a new column having datetime datatype and the content of label_4.\n",
    "data_df = data_df.withColumn(\"datetime\", to_timestamp(unix_timestamp(col('label_4'),\"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\").cast('timestamp')))\n",
    "\n",
    "## Dropping label_4 from the dataframe.\n",
    "data_df = data_df.drop(\"label_4\")\n",
    "\n",
    "## Print the data frame after adding and dropping a column.\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output:\n",
    "The Data part of the JSON file has been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of all the rows that are present in the dataframe obtained after the above modifications have been made.  \n",
    "Data = list(data_df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    type|\n",
      "+--------+\n",
      "| integer|\n",
      "|  string|\n",
      "| integer|\n",
      "|  string|\n",
      "|  string|\n",
      "|  number|\n",
      "|datetime|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Explode the fields part of the dataframe to obtain the data types of all the columns.\n",
    "col_list = df.select(explode(df[\"schema.fields\"])).toDF(\"level1\").select(\"level1.type\")\n",
    "col_list.show()\n",
    "\n",
    "## Extracting the above obtained datatypes to a list.\n",
    "datatype_list = [row[0] for row in col_list.select(\"type\").collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output:\n",
    "The datatypes from the Fields part of the JSON file have been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to obtain the datatype from the JSON data that is avaialble.\n",
    "def getType(raw):\n",
    "    switch = {  \n",
    "    \"byte\": ByteType(),\n",
    "    \"short\": ShortType(),\n",
    "    \"integer\": IntegerType(),\n",
    "    \"long\": LongType(),\n",
    "    \"float\": FloatType(),\n",
    "    \"number\": DoubleType(),\n",
    "    \"boolean\": BooleanType(),\n",
    "    \"datetime\": TimestampType(),\n",
    "    }\n",
    "    return switch.get(raw, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a schema containing column names as per the question and data types as per the JSON data that was available and finally extracted to a list in the above steps.\n",
    "## The getType function has been used to convert the field data available in string to datatype type.\n",
    "Schema = StructType([\n",
    "    StructField(\"content\", getType(datatype_list[1]),True),\n",
    "    StructField(\"label\", getType(datatype_list[2]),True),\n",
    "    StructField(\"size\", getType(datatype_list[3]),True),\n",
    "    StructField(\"usage\", getType(datatype_list[4]),True),\n",
    "    StructField(\"effect\", getType(datatype_list[5]),True),\n",
    "    StructField(\"date\", getType(datatype_list[6]),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- effect: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|             content|label|  size|   usage|      effect|               date|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|The battery is co...|    0| small|separate|0.7155163569|2015-06-05 18:41:08|\n",
      "|What a big waste ...|    0|medium|conected| 0.858630808|2016-10-29 12:12:46|\n",
      "|Don't waste your ...|    0| large|conected|0.2040485858|2016-04-29 14:44:31|\n",
      "|Great sound and s...|    1| large|separate| 0.332641236|2017-12-26 13:25:48|\n",
      "|Really pleased wi...|    1|medium|conected| 0.887390017|2016-04-30 00:01:08|\n",
      "|One of my favorit...|    1| large|conected|0.2305351126|2016-04-30 17:29:03|\n",
      "|best bluetooth on...|    1|medium|conected|0.4549175852|2017-04-24 04:26:54|\n",
      "|Authentic leather...|    1| large|conected|0.3198441525|2015-12-16 22:03:11|\n",
      "|I was very excite...|    1|medium|conected| 0.835863266|2015-05-19 01:34:19|\n",
      "|Do not make the s...|    0| small|conected|0.1442302304|2015-02-03 03:31:18|\n",
      "|Big Disappointmen...|    0| large|separate|0.8193389616|2017-11-26 06:01:00|\n",
      "|the phone was unu...|    0| large|separate| 0.136951624|2016-10-20 13:10:48|\n",
      "|Worst Customer Se...|    0| small|conected|0.8700157352|2015-11-09 10:40:50|\n",
      "|No additional ear...|    0| large|separate| 0.719855014|2017-02-17 19:49:15|\n",
      "|It defeats the pu...|    0| small|conected|0.7345972022|2015-06-22 01:20:10|\n",
      "|  Worth every penny.|    1| large|separate|0.5062303093|2017-09-15 10:03:16|\n",
      "|Excellent wallet ...|    1| small|conected|0.4407609104|2015-06-27 00:04:24|\n",
      "|Nice headphones f...|    1|medium|conected|0.5066209266|2017-01-01 00:32:42|\n",
      "|Internet is excru...|    0|medium|conected|0.5714815553|2015-02-06 17:18:37|\n",
      "|It is very comfor...|    1| small|separate|0.9208235177|2015-04-30 03:07:00|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create a dataframe by combining the list of rows and the schema that have \n",
    "final_df = spark.createDataFrame(data=Data, schema=Schema)\n",
    "\n",
    "## Print the schema of the data frame that has been created with the modified column names and data types.\n",
    "final_df.printSchema()\n",
    "\n",
    "## Print the data frame that has thus been obtained.\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output:\n",
    "The final dataframe has been created by obtaining the datatypes and the data from the JSON file. The column names haves have been altered as per the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new temporary view using a SparkDataFrame in the Spark Session. \n",
    "## If a temporary view with the same name already exists, it is replaced.\n",
    "final_df.createOrReplaceTempView(\"DM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A temporary view has been created which will further be used for querying the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "Group the table by 'size', and sort based on 'date'.  Then, create a new column that is the difference between 'date' in consecutive records (within groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- effect: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- diff_date: string (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+--------+------------+-------------------+--------------------+\n",
      "|             content|label|  size|   usage|      effect|               date|           diff_date|\n",
      "+--------------------+-----+------+--------+------------+-------------------+--------------------+\n",
      "|Better than you'd...|    1|medium|separate| 0.049132322|2015-01-01 23:54:55|                null|\n",
      "|I love the look a...|    1|medium|separate|0.5040957229|2015-01-04 04:11:57|52 hours 17 minut...|\n",
      "|lightweight and w...|    1|medium|conected|0.2213363547|2015-01-13 15:30:56|227 hours 18 minu...|\n",
      "|EXCELLENT SERVICE...|    1|medium|separate|0.6684907436|2015-01-14 23:04:14|31 hours 33 minut...|\n",
      "|Everything about ...|    1|medium|conected|0.1309561045|2015-01-15 20:07:22|21 hours 3 minute...|\n",
      "|I get absolutely ...|    0|medium|separate|0.9481961624|2015-01-16 06:24:17|10 hours 16 minut...|\n",
      "|Five star Plus, p...|    1|medium|conected|0.1540949024|2015-01-17 22:36:15|40 hours 11 minut...|\n",
      "|The range is very...|    1|medium|separate|0.0240287714|2015-01-28 01:30:35|242 hours 54 minu...|\n",
      "|My father has the...|    0|medium|conected|0.2127243987|2015-01-30 18:12:41|64 hours 42 minut...|\n",
      "|   It was horrible!.|    0|medium|conected|0.0528493387|2015-01-31 19:50:15|25 hours 37 minut...|\n",
      "|Light weight, I h...|    1|medium|separate|0.4133420385|2015-02-02 14:57:52|43 hours 7 minute...|\n",
      "|   Never got it!!!!!|    0|medium|separate|0.9879631688|2015-02-03 04:55:47|13 hours 57 minut...|\n",
      "|Internet is excru...|    0|medium|conected|0.5714815553|2015-02-06 17:18:37|84 hours 22 minut...|\n",
      "|horrible, had to ...|    0|medium|separate|0.6361942279|2015-02-15 23:23:28|222 hours 4 minut...|\n",
      "|Lasted one day an...|    0|medium|conected|0.9875535516|2015-02-20 04:54:09|101 hours 30 minu...|\n",
      "|Love it.. Great a...|    1|medium|separate|0.8991056524|2015-02-21 16:48:09| 35 hours 54 minutes|\n",
      "|Their Research an...|    1|medium|separate|0.4134599587|2015-02-24 15:18:05|70 hours 29 minut...|\n",
      "|Sound quality on ...|    1|medium|separate| 0.047711758|2015-02-26 02:36:24|35 hours 18 minut...|\n",
      "|If you like a lou...|    0|medium|separate|0.0933155848|2015-02-27 20:12:57|41 hours 36 minut...|\n",
      "|         Worst ever.|    0|medium|separate|0.7570866283|2015-03-03 17:04:44|92 hours 51 minut...|\n",
      "+--------------------+-----+------+--------+------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Grouping the table by 'size', and sort based on 'date' and extract difference between 'date' in consecutive records'\n",
    "groupedby =spark.sql(\"SELECT *, date - lag(date,1) OVER(PARTITION BY size order by date) as diff_date FROM DM\")\n",
    "\n",
    "## Creating a new column that is the difference between 'date' in consecutive records (within groups).\n",
    "groupedby = groupedby.withColumn(\"diff_date\", col(\"diff_date\").cast('String'))\n",
    "groupedby.printSchema()\n",
    "groupedby.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output:\n",
    "The data has been grouped by size and sorted by the date. A new column has been added that contains the difference between 'date' in consecutive records (within groups)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save\n",
    "\n",
    "Save the data results as one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the above obtained data into a CSV file. The level of parallelism can be increased for better efficiency by changing the parameter in repartition option.\n",
    "groupedby.repartition(1).write.format('csv').save(\"./output/Manipulated_data.csv\", header = 'true', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results after the data frame creation and manipulation have been saved to Manipulated_data.csv file in the Output folder.\n",
    "#### Future Works:\n",
    "This dataframe that has been obtained can be queried and manipulated in multiple different ways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
